{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea140ec7",
   "metadata": {
    "id": "ea140ec7"
   },
   "source": [
    "# ECC4200 Visual Computing and Machine Perception Project #1\n",
    "\n",
    "In this project, you are going to start an exciting journey to explore Deep Learning and Neural Networks by completing the following three tasks:\n",
    "\n",
    "* **Task 1. Understanding and conducting convolution (8%).**\n",
    "* **Task 2. Building and training a ResNet18 model (52%).**\n",
    "* **Task 3. Exploring and explaining the trained model (40%).**\n",
    "\n",
    "Before doing the project, please read the instructions carefully (failure to do so will be penalized):\n",
    "\n",
    "1. Implement your codes **within** \"TODO\" and \"END OF YOUR CODE\", do **NOT** modify any codes outside the answer area;\n",
    "2. Make sure your codes **clean**, **easily readable** (add meaningful comments if needed), and **runnable**;\n",
    "3. Write your answers in the given markdown cells, keep your answers clear and concise;\n",
    "4. Do submit your project before the deadline: **31 March, 23:59 SGT (Singapore Time)**. The deadline is strict. **Late submission will be deducted 10 points (out of 100) for every 24 hours**;\n",
    "5. Once you finish the tasks, **compress the finished notebook (.ipynb file) and the saved model (model_resnet18.pt file) into a zip file**, and then **submit the zip file named as \"StudentID_Name_ECC4200_project1.zip\"**. Make sure that the **submitted notebook has been run** and the **cell outputs are visible**;\n",
    "6. This is an individual project, do **NOT** share your solutions with others, we have zero tolerance for cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f0884",
   "metadata": {
    "id": "222f0884"
   },
   "source": [
    "Note:\n",
    "\n",
    "1. It is recommended to do model training on GPU, you may use [Colab](https://colab.google/) or [Kaggle](https://www.kaggle.com/) for free computing resources for faster training.\n",
    "2. Since free computing resources accessible on [Colab](https://colab.google/) is limited for unsuscribed users for each day, it is recommended to debug on your CPU first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc14d01",
   "metadata": {
    "id": "ccc14d01"
   },
   "source": [
    "## Task 1: Understanding and conducting convolution (8%)\n",
    "**Subtasks**\n",
    "1. Use numpy to conduct 2D standard convolution operation (4%).\n",
    "2. Use numpy to conduct 2D dilated convolution operation (4%).\n",
    "\n",
    "You may refer to *[\"A guide to convolution arithmetic for deeplearning\"](https://arxiv.org/pdf/1603.07285.pdf)* for details of dilated convolution.\n",
    "\n",
    "In this task, we follow the settings as in the paper:\n",
    "* 2-D discrete convolutions,\n",
    "* square inputs,\n",
    "* square kernel size,\n",
    "* same strides along both axes,\n",
    "* same zero padding along both axes.\n",
    "\n",
    "Score points:\n",
    "1. The implementations are correct.\n",
    "2. The implementation of dilated convolution makes use of the relationship between dilated convolution kernel and standard convolution kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7279fa95",
   "metadata": {
    "id": "7279fa95"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e75e78",
   "metadata": {
    "id": "c0e75e78"
   },
   "outputs": [],
   "source": [
    "def conv2d(inputs,kernels,padding=0, stride=1):\n",
    "    assert inputs.shape[0] == kernels.shape[1], \"The numbers of channels of input and kernel do not match.\"\n",
    "    ##############################################################################\n",
    "    # TODO: conduct convolution calculation based on given inputs, kernels,      #\n",
    "    #       padding and stride values.                                           #\n",
    "    ##############################################################################\n",
    "    # your code\n",
    "    \n",
    "    # 计算输出数组的形状\n",
    "    outputs_height = (inputs.shape[1] + 2 * padding - (kernels.shape[2] - 1)) // stride + 1\n",
    "    outputs_width = (inputs.shape[2] + 2 * padding - (kernels.shape[3] -1 )) // stride + 1\n",
    "    \n",
    "    # 初始化输出数组\n",
    "    outputs = np.zeros((kernels.shape[0], outputs_height, outputs_width))\n",
    "    \n",
    "    # 执行二维标准卷积运算\n",
    "    for channel in range(kernels.shape[0]):\n",
    "        for i in range(outputs_height):\n",
    "            for j in range(outputs_width):\n",
    "                # 计算输入数据对应区域的索引范围\n",
    "                input_i_start = i * stride\n",
    "                input_i_end = input_i_start + kernels.shape[2]\n",
    "                input_j_start = j * stride\n",
    "                input_j_end = input_j_start + kernels.shape[3]\n",
    "                \n",
    "                # 切片输入数据，并与卷积核进行乘法运算\n",
    "                outputs[channel, i, j] = np.sum(inputs[:, input_i_start:input_i_end, input_j_start:input_j_end] * kernels[channel, :, :, :])\n",
    "    \n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56dc334c",
   "metadata": {
    "id": "56dc334c"
   },
   "outputs": [],
   "source": [
    "def dilated_conv2d(inputs,kernels,dilation,padding=0,stride=1):\n",
    "    assert inputs.shape[0] == kernels.shape[1], \"The numbers of channels of input and kernel do not match.\"\n",
    "    ##############################################################################\n",
    "    # TODO: conduct dilated convolution calculation based on given inputs,       #\n",
    "    #       kernels, padding and stride values.                                  #\n",
    "    ##############################################################################\n",
    "    # your code\n",
    "\n",
    "   # 计算输出数组的形状\n",
    "    outputs_height = (inputs.shape[1] + 2 * padding - (kernels.shape[2] - 1) * dilation) // stride + 1\n",
    "    outputs_width = (inputs.shape[2] + 2 * padding - (kernels.shape[3] - 1) * dilation) // stride + 1\n",
    "\n",
    "    # 初始化输出数组\n",
    "    outputs = np.zeros((kernels.shape[0], outputs_height, outputs_width))\n",
    "\n",
    "    # 执行二维标准卷积运算\n",
    "    for channel in range(kernels.shape[0]):\n",
    "        for i in range(outputs_height):\n",
    "            for j in range(outputs_width):\n",
    "                # 计算卷积核的填充位置\n",
    "                pad_top = max(0, (kernels.shape[2] - 1) * dilation - i * stride)\n",
    "                pad_bottom = max(0, i * stride)\n",
    "                pad_left = max(0, (kernels.shape[3] - 1) * dilation - j * stride)\n",
    "                pad_right = max(0, j * stride)\n",
    "\n",
    "                # 对卷积核进行填充\n",
    "                padded_kernel = np.pad(kernels[channel, :, :, :], ((0, 0), (pad_top, pad_bottom), (pad_left, pad_right)), mode='constant')\n",
    "\n",
    "                # 计算卷积运算结果\n",
    "                input_i_start = max(0, i * stride - pad_top)\n",
    "                input_i_end = min(inputs.shape[1], input_i_start + padded_kernel.shape[1])\n",
    "                input_j_start = max(0, j * stride - pad_left)\n",
    "                input_j_end = min(inputs.shape[2], input_j_start + padded_kernel.shape[2])\n",
    "                # 切片输入数据，并与填充后的卷积核进行乘法运算\n",
    "                outputs[channel, i, j] = np.sum(inputs[:, input_i_start:input_i_end, input_j_start:input_j_end] * padded_kernel[:, :input_i_end - input_i_start, :input_j_end - input_j_start])\n",
    "\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65be16f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65be16f9",
    "outputId": "7da2d4be-3843-480c-cf60-e6d85abf127d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "[[[0 2 0 0 0]\n",
      "  [2 1 1 0 2]\n",
      "  [2 0 1 0 1]\n",
      "  [2 2 2 0 2]\n",
      "  [2 1 2 2 0]]\n",
      "\n",
      " [[2 1 2 2 1]\n",
      "  [1 1 2 0 1]\n",
      "  [2 2 2 0 2]\n",
      "  [0 2 1 2 2]\n",
      "  [0 0 1 2 1]]]\n",
      "\n",
      "Kernels:\n",
      "[[[[0 0]\n",
      "   [1 2]]\n",
      "\n",
      "  [[2 0]\n",
      "   [2 0]]]\n",
      "\n",
      "\n",
      " [[[1 1]\n",
      "   [2 2]]\n",
      "\n",
      "  [[0 0]\n",
      "   [0 1]]]\n",
      "\n",
      "\n",
      " [[[1 0]\n",
      "   [1 1]]\n",
      "\n",
      "  [[1 1]\n",
      "   [0 0]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2024)\n",
    "inputs = np.random.randint(0,3,size=(2,5,5))\n",
    "kernels = np.random.randint(0,3,size=(3,2,2,2))\n",
    "print(f\"Inputs:\\n{inputs}\\n\")\n",
    "print(f\"Kernels:\\n{kernels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "869672ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "869672ff",
    "outputId": "f0da849a-cfe3-4adf-85e2-4ceaff79fda9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize inputs\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "for i in range(inputs.shape[0]):\n",
    "    ax = fig.add_subplot(1, inputs.shape[0], i+1, xticks=[], yticks=[])\n",
    "    ax.set_title('Input channel %s' % str(i+1))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.grid(True)\n",
    "    width = inputs[i].shape[0]\n",
    "    height = inputs[i].shape[1]\n",
    "\n",
    "\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(nbins=width))\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(nbins=height))\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            ax.annotate(str(inputs[i][x][y]), xy=(y/height+(0.5/height),x/width+(0.5/height)),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "197b7b25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "197b7b25",
    "outputId": "9a74d9be-a048-4f6c-9b76-63494d20723c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1200 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize kernels\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 12))\n",
    "for i in range(kernels.shape[0]):\n",
    "    for j in range(kernels.shape[1]):\n",
    "        ax = fig.add_subplot(kernels.shape[0], kernels.shape[1], i*kernels.shape[1] + j+1, xticks=[], yticks=[])\n",
    "        ax.set_title(f'kernel {str(i+1)}, channel {str(j+1)}')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.grid(True)\n",
    "        width = kernels[i][j].shape[0]\n",
    "        height = kernels[i][j].shape[1]\n",
    "\n",
    "\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(nbins=width))\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(nbins=height))\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                ax.annotate(str(kernels[i][j][x][y]), xy=(y/height+(0.5/height),x/width+(0.5/height)),\n",
    "                            horizontalalignment='center',\n",
    "                            verticalalignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f64868dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f64868dd",
    "outputId": "2ce8aeb8-ebf3-4f28-c678-ebcdd8c6e8f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[10.,  7.,  9.,  8., 10.],\n",
       "        [ 8.,  8.,  9.,  2.,  9.],\n",
       "        [10., 14.,  8.,  8., 14.],\n",
       "        [ 4.,  9., 10., 10.,  6.],\n",
       "        [ 4.,  5., 10., 10.,  4.]],\n",
       "\n",
       "       [[ 9.,  8.,  2.,  5.,  9.],\n",
       "        [ 9.,  6.,  3.,  6., 10.],\n",
       "        [12., 10.,  7.,  7., 12.],\n",
       "        [10., 11., 12.,  7.,  5.],\n",
       "        [ 9., 10., 14.,  7.,  1.]],\n",
       "\n",
       "       [[ 6.,  7.,  5.,  5.,  6.],\n",
       "        [ 6.,  5.,  4.,  2.,  6.],\n",
       "        [10.,  8.,  5.,  4.,  9.],\n",
       "        [ 7.,  8.,  9.,  6.,  6.],\n",
       "        [ 5.,  5.,  9.,  7.,  2.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d(inputs,kernels,stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b85f26e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b85f26e",
    "outputId": "ce094e04-ecfc-4fc0-b614-e1dbdc2540d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 8., 14.,  8.,  8.],\n",
       "        [ 9.,  8.,  9.,  2.],\n",
       "        [ 8., 14.,  8.,  8.],\n",
       "        [10.,  9., 10., 10.]],\n",
       "\n",
       "       [[ 7., 10.,  7.,  7.],\n",
       "        [ 3.,  6.,  3.,  6.],\n",
       "        [ 7., 10.,  7.,  7.],\n",
       "        [12., 11., 12.,  7.]],\n",
       "\n",
       "       [[ 5.,  8.,  5.,  4.],\n",
       "        [ 4.,  5.,  4.,  2.],\n",
       "        [ 5.,  8.,  5.,  4.],\n",
       "        [ 9.,  8.,  9.,  6.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dilated_conv2d(inputs,kernels,dilation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0838d",
   "metadata": {
    "id": "7bb0838d"
   },
   "source": [
    "## Task 2: Building and training a ResNet18 model (52%)\n",
    "**Subtasks**\n",
    "1. Figure out the ResNet18 model architecture and write down the dimension of features of each layer (4%).\n",
    "2. Build a ResNet18 model by PyTorch (20%).\n",
    "3. Complete the codes to evaluate and train the model. (10%)\n",
    "4. Viusalize the training curves (2%).\n",
    "5. Point out a potential problem in the training process (2%), propose possible solutions (4%) to improve and implement **at least ONE** of them (10%).\n",
    "\n",
    "Score points:\n",
    "1. The results for subtask 1 is correct.\n",
    "2. The implementation for subtask 2 is correct.\n",
    "3. The implementation for subtask 2 is also concise, i.e., building the network block by block instead of layer by layer.\n",
    "4.\n",
    "5. The two plots for visualization contain all information of interest, one for loss curves and another for accuracy curves.\n",
    "6. The answers to subtask 5 are reasonable and the implementation is correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4031c",
   "metadata": {
    "id": "0ad4031c"
   },
   "source": [
    "### Prepare packages and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f4bb2d7",
   "metadata": {
    "id": "9f4bb2d7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ec7314",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91ec7314",
    "outputId": "9a03681e-3b2a-46bc-d60c-3b3bfc917ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "train_size = len(train_set)\n",
    "print(train_size)\n",
    "test_size = len(test_set)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cecc458",
   "metadata": {
    "id": "3cecc458"
   },
   "source": [
    "### Define the ResNet18 model\n",
    "\n",
    "Residual Network (ResNet) is a deep learning model widely used for computer vision applications. It is a Convolutional Neural Network (CNN) architecture. ResNet provides an innovative solution to the vanishing gradient problem, known as “residual connections”.\n",
    "\n",
    "ResNet includes multiple \"residual blocks\", each of which contains certain types of layers and residual connections as shown in the figure below.\n",
    "\n",
    "Refer to the paper *[\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/pdf/1512.03385.pdf)* for information about ResNet architecture.\n",
    "\n",
    "Figure out the architecture of ResNet18 and the feature dimension of each layer, considering the input dimension as `(3,32,32)` representing `(C, H, W)`. Write down the architecture and the feature dimension corresponding to each layer in the following markdown cell.\n",
    "\n",
    "Implement to define a ResNet18 model in the following code block.\n",
    "\n",
    "Note that while doing residual connection, if the dimensions of the input and output of the residual block are not the same, we adjust channels and resolution of the input by means of a convolution before adding.\n",
    "\n",
    "![](resnet-block.svg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6147ce",
   "metadata": {},
   "source": [
    "---\n",
    "**Write down your results of the ResNet18 architecture and specify the feature dimension for each layer.**\n",
    "Here are the results of the ResNet18 architecture and the feature dimensions for each layer:\n",
    "\n",
    "1. Input layer:\n",
    "   - Shape: (3, 32, 32)\n",
    "   - Explanation: RGB image with 32x32 resolution.\n",
    "\n",
    "2. Convolutional layer (conv1):\n",
    "   - Number of filters: 64\n",
    "   - Kernel size: 7x7\n",
    "   - Stride: 2\n",
    "   - Padding: 3\n",
    "   - Output shape: (64, 16, 16)\n",
    "   - Explanation: 64 filters with 7x7 kernel size, followed by stride 2 and padding 3, resulting in output feature maps of size 16x16.\n",
    "\n",
    "3. Residual blocks (layer1, layer2, layer3, layer4):\n",
    "   - Each block contains two residual units (basic blocks).\n",
    "   - Basic block structure:\n",
    "     - Convolutional layer 1: 64 filters, 3x3 kernel size, stride 1, padding 1\n",
    "     - Batch normalization\n",
    "     - ReLU activation\n",
    "     - Convolutional layer 2: 64 filters, 3x3 kernel size, stride 1, padding 1\n",
    "     - Batch normalization\n",
    "     - Residual connection: Identity mapping (no change in dimension)\n",
    "   - Output shape after each layer:\n",
    "     - layer1: (64, 16, 16)\n",
    "     - layer2: (128, 8, 8)\n",
    "     - layer3: (256, 4, 4)\n",
    "     - layer4: (512, 2, 2)\n",
    "\n",
    "4. Average pooling layer (avg_pool):\n",
    "   - Kernel size: 4x4\n",
    "   - Stride: 4\n",
    "   - Output shape: (512, 1, 1)\n",
    "   - Explanation: Global average pooling reduces each 2x2 feature map to a single value, resulting in a feature vector of size 512x1x1.\n",
    "\n",
    "5. Fully connected layer (fc):\n",
    "   - Input size: 512\n",
    "   - Output size: Number of classes (10 in CIFAR-10)\n",
    "   - Explanation: Linear layer for classification.\n",
    "\n",
    "Overall, ResNet18 gradually reduces spatial dimensions while increasing the number of filters, leading to hierarchical feature extraction culminating in a classification output.\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcc26789",
   "metadata": {
    "id": "dcc26789"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 strides = 1):\n",
    "        '''\n",
    "        input_channels: the number of channels of input x.\n",
    "        num_channels: the number of channels channels of the output of the residual block.\n",
    "        strides: the strides for the first convolutional layer in the residual block, \n",
    "                 note that this is not applied to the second convolutional layer in the residual block.\n",
    "        '''\n",
    "        ##############################################################################\n",
    "        # TO DO: Define a ResidualBlock module as the figure shown above.            #\n",
    "        ##############################################################################\n",
    "        #your code\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, stride=strides, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        self.downsample = None\n",
    "        if strides != 1 or input_channels != num_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides, bias=False),\n",
    "                nn.BatchNorm2d(num_channels)\n",
    "            )\n",
    "\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        ##############################################################################\n",
    "        # TO DO: implement the forward path of the ResidualBlock module.             #\n",
    "        ##############################################################################\n",
    "        #your code\n",
    "\n",
    "        identity = X\n",
    "        out = self.conv1(X)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(X)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a90980",
   "metadata": {
    "id": "a7a90980"
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    ##############################################################################\n",
    "    # TO DO: Define a ResNet18 model and implement its forward path, you may     #\n",
    "    #        also add other functions to this class if necessary.                #\n",
    "    ##############################################################################\n",
    "    # your code\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
    "        self.layer1 = self._make_layer(64, 64, 2)\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.conv1(X)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d130b5b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d130b5b1",
    "outputId": "d116ddcc-1d9e-4ec8-e63d-e6a7123b309a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9c109",
   "metadata": {
    "id": "85a9c109"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ba956d73",
   "metadata": {
    "id": "ba956d73"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Build data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True, num_workers = 0)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = False, num_workers = 0)\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"test\": test_loader}\n",
    "dataset_sizes = {\"train\": train_size, \"test\": test_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c40ce7",
   "metadata": {
    "id": "94c40ce7"
   },
   "source": [
    "Write a functions to evaluate the model on testing set and train the model for one epoch in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b53d0326",
   "metadata": {
    "id": "b53d0326"
   },
   "outputs": [],
   "source": [
    "def eval_on_test_set(model):\n",
    "    model.eval()\n",
    "    device = torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    running_accuracy = 0\n",
    "    loss = 0\n",
    "\n",
    "    for data in test_loader:\n",
    "        ##############################################################################\n",
    "        # TODO: Implement the evaluation process on test set.                        #\n",
    "        ##############################################################################\n",
    "        # your code\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(inputs)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += batch_loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        running_accuracy += correct\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "    total_loss=loss/test_size\n",
    "    total_accuracy = running_accuracy / test_size\n",
    "    print('Evaluation on test set: loss={:.3f} \\t accuracy={:.2f}%'.format(total_loss, total_accuracy * 100))\n",
    "    model.train()\n",
    "    return total_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "462aea7e",
   "metadata": {
    "id": "462aea7e"
   },
   "outputs": [],
   "source": [
    "def train_for_one_epoch(model):\n",
    "    model.train()\n",
    "    # Set up device\n",
    "    device = torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device {device} to train the model.\")\n",
    "    model.to(device)\n",
    "\n",
    "    # set the running quatities to zero at the beginning of the epoch\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        ##############################################################################\n",
    "        # TODO: Implement the training process for one epoch.                        #\n",
    "        ##############################################################################\n",
    "        # your code\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # compute running statistics\n",
    "        running_loss += batch_loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        running_accuracy += correct\n",
    "\n",
    "\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "\n",
    "    # Compute stats for the full training set\n",
    "    total_loss = running_loss / train_size\n",
    "    total_accuracy = running_accuracy / train_size\n",
    "\n",
    "    return total_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ffabe5b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffabe5b1",
    "outputId": "d7ef96d9-c89e-4a16-a1f6-bcc779a09bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu to train the model.\n",
      "Training epoch= 0 \t cost_time= 3.24 min \t loss= 0.006 \t accuracy= 42.67%\n",
      "Evaluation  on test set: loss0.009 \t accuracy = 37.80%\n",
      "Using device cpu to train the model.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[206], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      5\u001b[0m   start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m   train_loss_epoch, train_acc_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_for_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m   elapsed \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining epoch= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m cost_time= \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m min \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m loss= \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m accuracy= \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, elapsed, train_loss_epoch, train_acc_epoch \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m))\n",
      "Cell \u001b[1;32mIn[205], line 21\u001b[0m, in \u001b[0;36mtrain_for_one_epoch\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     20\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mbatch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# compute running statistics\u001b[39;00m\n",
      "File \u001b[1;32me:\\conda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# start training\n",
    "\n",
    "metrics = {\"train_loss\":[], \"train_acc\":[], \"test_loss\":[], \"test_acc\":[]}\n",
    "for epoch in range(epochs):\n",
    "  start=time.time()\n",
    "  train_loss_epoch, train_acc_epoch = train_for_one_epoch(model)\n",
    "  elapsed = (time.time()-start) / 60\n",
    "  print('Training epoch= {} \\t cost_time= {:.2f} min \\t loss= {:.3f} \\t accuracy= {:.2f}%'.format(epoch, elapsed, train_loss_epoch, train_acc_epoch * 100))\n",
    "  test_loss_epoch, test_acc_epoch = eval_on_test_set(model)\n",
    "  metrics['train_loss'].append(train_loss_epoch)\n",
    "  metrics['train_acc'].append(train_acc_epoch)\n",
    "  metrics['test_loss'].append(test_loss_epoch)\n",
    "  metrics['test_acc'].append(test_acc_epoch)\n",
    "\n",
    "# save your trained model for the following question\n",
    "torch.save(model.state_dict(), './model_resnet18.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf18256",
   "metadata": {
    "id": "0bf18256"
   },
   "source": [
    "Visualize the training curves for loss and accuracy in the following code block. Your figure should include two subplots, one for loss curves on training and testing sets, and another for accuracy curves on training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0h001tQKPq_w",
   "metadata": {
    "id": "0h001tQKPq_w"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# TODO: Visualize the loss curves and accuracy curves on training and         #\n",
    "#       testing sets respectively during training.                            #\n",
    "###############################################################################\n",
    "\n",
    "# your code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loss = metrics['train_loss']\n",
    "test_loss = metrics[\"test_loss\"]\n",
    "train_accuracy = metrics['train_acc']\n",
    "test_accuracy = metrics['test_acc']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# 创建一个包含两个子图的画布\n",
    "fig, axs = plt.subplots(2, figsize=(12, 12))\n",
    "\n",
    "# 绘制训练集和测试集的损失曲线\n",
    "axs[0].plot(epochs, train_loss, 'b', label='Training loss')\n",
    "axs[0].plot(epochs, test_loss, 'r', label='Testing loss')\n",
    "axs[0].set_title('Loss Curves')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# 绘制训练集和测试集的准确率曲线\n",
    "axs[1].plot(epochs, train_accuracy, 'b', label='Training accuracy')\n",
    "axs[1].plot(epochs, test_accuracy, 'r', label='Testing accuracy')\n",
    "axs[1].set_title('Accuracy Curves')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "# 显示图形\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iG9yxN-VReRW",
   "metadata": {
    "id": "iG9yxN-VReRW"
   },
   "source": [
    "### Improve the ResNet18 trained above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jQdHY3bgVqEf",
   "metadata": {
    "id": "jQdHY3bgVqEf"
   },
   "source": [
    "Observe the loss and accuracy curves during training and testing respectively, what potential problem can be concluded if continue training the model for further epochs most probably? What kind of techniques can be applied to solve this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KRhf5vVOVtMj",
   "metadata": {
    "id": "KRhf5vVOVtMj"
   },
   "source": [
    "---\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "If we observe the loss and accuracy curves during training and testing for the ResNet18 model with 10 epochs and a learning rate of 0.1, one potential problem that may arise if we continue training the model for further epochs is overfitting. Overfitting occurs when the model learns to perform well on the training data but does not generalize well to unseen data, leading to a decrease in test accuracy and an increase in test loss.\n",
    "\n",
    "To address the potential problem of overfitting when continuing training the model for further epochs, several techniques can be applied:\n",
    "\n",
    "1. **Regularization Techniques:** Use regularization techniques such as L2 regularization (weight decay) or dropout. These techniques help prevent overfitting by adding penalties to the model's weights or randomly dropping units during training.\n",
    "\n",
    "2. **Data Augmentation:** Augment the training data by applying transformations such as random cropping, horizontal flipping, or rotation. This increases the diversity of the training samples and helps the model generalize better.\n",
    "\n",
    "3. **Early Stopping:** Monitor the validation (or test) loss during training and stop training when the validation loss starts to increase, indicating that the model is overfitting the training data.\n",
    "\n",
    "4. **Reduce Learning Rate:** Implement learning rate scheduling techniques such as reducing the learning rate over time (e.g., using a learning rate scheduler or manually reducing the learning rate) to allow the model to converge more slowly and prevent overfitting.\n",
    "\n",
    "5. **Use a Larger Dataset:** If possible, use a larger dataset to train the model. More data can help the model learn a more generalized representation of the underlying patterns in the data, reducing the risk of overfitting.\n",
    "\n",
    "By applying these techniques, we can mitigate the risk of overfitting and improve the model's generalization performance on unseen data.\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BSaYUOtQRsNC",
   "metadata": {
    "id": "BSaYUOtQRsNC"
   },
   "source": [
    "Please choose one technique you mentioned above and implement it, retrain the model, observe and report the loss and accuracy again (10%).\n",
    "\n",
    "**Note: you are not expected to save your improved model with the name \"model_resnet18.pt\" in this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QLxc7yGKRwv5",
   "metadata": {
    "id": "QLxc7yGKRwv5"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Choose a technique to improve your model.                            #\n",
    "##############################################################################\n",
    "# your code\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "metrics = {\"train_loss\":[], \"train_acc\":[], \"test_loss\":[], \"test_acc\":[]}\n",
    "for epoch in range(epochs):\n",
    "  start=time.time()\n",
    "  train_loss_epoch, train_acc_epoch = train_for_one_epoch(model)\n",
    "  elapsed = (time.time()-start) / 60\n",
    "  print('Training epoch={} \\t cost_time={:.2f} min \\t loss={:.3f} \\t accuracy={:.2f}%'.format(epoch, elapsed, train_loss_epoch, train_acc_epoch * 100))\n",
    "  test_loss_epoch, test_acc_epoch = eval_on_test_set(model)\n",
    "  metrics['train_loss'].append(train_loss_epoch)\n",
    "  metrics['train_acc'].append(train_acc_epoch)\n",
    "  metrics['test_loss'].append(test_loss_epoch)\n",
    "  metrics['test_acc'].append(test_acc_epoch)\n",
    "\n",
    "# save your trained model for the following question\n",
    "torch.save(model.state_dict(), './model_resnet18_improve.pt')\n",
    "\n",
    "train_loss = metrics['train_loss']\n",
    "test_loss = metrics[\"test_loss\"]\n",
    "train_accuracy = metrics['train_acc']\n",
    "test_accuracy = metrics['test_acc']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# 创建一个包含两个子图的画布\n",
    "fig, axs = plt.subplots(2, figsize=(12, 12))\n",
    "\n",
    "# 绘制训练集和测试集的损失曲线\n",
    "axs[0].plot(epochs, train_loss, 'b', label='Training loss')\n",
    "axs[0].plot(epochs, test_loss, 'r', label='Testing loss')\n",
    "axs[0].set_title('Loss Curves')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend()\n",
    "\n",
    "# 绘制训练集和测试集的准确率曲线\n",
    "axs[1].plot(epochs, train_accuracy, 'b', label='Training accuracy')\n",
    "axs[1].plot(epochs, test_accuracy, 'r', label='Testing accuracy')\n",
    "axs[1].set_title('Accuracy Curves')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend()\n",
    "\n",
    "# 显示图形\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EpPWdAT_R16F",
   "metadata": {
    "id": "EpPWdAT_R16F"
   },
   "source": [
    "---\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e084bfd",
   "metadata": {
    "id": "3e084bfd"
   },
   "source": [
    "## Task 3: Exploring and explaining the trained model (40%)\n",
    "\n",
    "**Subtasks:**\n",
    "\n",
    "1. Visualize the representations for bottom and top layers by t-SNE, compare and make conclusion (6%).\n",
    "2. Compute saliency map and answer the question about it (6%).\n",
    "3. Compute improved saliency map by SMOOTHGRAD and answer the question about the comparison between the saliency map in subtask 2 and SMOOTHGRAD (10%).\n",
    "4. Design and conduct experiment to explain how CNN works using SMOOTHGRAD (12%).\n",
    "5. Given a model, generate fooling image based on an original image and a target label to fool. Write down the observations from the result (6%).\n",
    "\n",
    "Score points:\n",
    "1. For subtask 1, recognize the correct layers of insterest, extract corresponding intermediate features and make reasonable conclusion.\n",
    "2. For subtask 2, the implementation is correct and as concise as possible and the question is correctly answered.\n",
    "3. For subtask 3, the implementation is correct and as concise as possible and the question is correctly answered.\n",
    "4. For subtask 4, the experiment is reasonably designed and appropriately conducted.\n",
    "5. For subtask 5, the implementation is correct and observation is reasonable.\n",
    "\n",
    "**<mark>Highlightedd Note: for task 3, just use the model trained and saved in task 2 part \"define the ResNet18 model\" instead of your improved model in task 2 part \"improve the ResNet18\".<mark>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y875npg6WpU0",
   "metadata": {
    "id": "Y875npg6WpU0"
   },
   "source": [
    "### Load the trained ResNet18 model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377794e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "377794e5",
    "outputId": "bb5c7df1-b5b8-4350-d888-8e85e794eb82"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet18' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mResNet18\u001b[49m()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./model_resnet18.pt\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain the model first\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load the model trained and saved in task 2 part \"define the ResNet18 model\" \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ResNet18' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import matplotlib.cm as cm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "trained_model = ResNet18()\n",
    "assert os.path.exists('./model_resnet18.pt'), 'train the model first'\n",
    "# Load the model trained and saved in task 2 part \"define the ResNet18 model\" \n",
    "trained_model.load_state_dict(torch.load('./model_resnet18.pt', map_location=torch.device('cpu'))) \n",
    "\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "trained_model.to(device)\n",
    "trained_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7jIOMxcocHIU",
   "metadata": {
    "id": "7jIOMxcocHIU"
   },
   "source": [
    "### Visualize the learned features for the trained ResNet18 of different layers of model.\n",
    "\n",
    "t-SNE (t-distributed Stochastic Neighbor Embedding) is an unsupervised non-linear dimensionality reduction technique for data exploration and visualizing high-dimensional data. Here, you shall apply t-SNE to the features extracted from a bottom layer and a top layer of the trained ResNet18 model.\n",
    "\n",
    "You should complete:\n",
    "* 1) extract features for the bottom layer and top layer respectively, i.e., the intermediate outputs of these layers.\n",
    "* 2) if the extracted features are in form of feature maps, reshape the feature map for each sample to make it a vector.\n",
    "* 3) visualize the features for the bottom and top layers by t-SNE, observe and analyze the results.\n",
    "\n",
    "The bottom layer is defined as the first max-pooling layer of the whole model; the top layer is defined as the penultimate layer of the whole model. (We refer to the input side as \"bottom\" and the output side as \"top\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OGK1bzp8cUD2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "OGK1bzp8cUD2",
    "outputId": "a809500b-3dd9-4465-c089-bf43edc9fd59"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TO DO: Extract intermediate features of the top and bottom layers          #\n",
    "#        based on your ResNet18 model.                                       #\n",
    "##############################################################################\n",
    "# Assuming you have a ResNet18 model named 'model' and a dataloader 'train_loader'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Define lists to store features\n",
    "features_top = []\n",
    "features_bottom = []\n",
    "labels = []\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Iterate through the data loader to extract features\n",
    "with torch.no_grad():\n",
    "    for images, targets in train_loader:\n",
    "        outputs = model(images)\n",
    "        features_top.append(outputs['avgpool'].numpy())  # Extract top layer features\n",
    "        features_bottom.append(outputs['layer1'].numpy())  # Extract bottom layer features\n",
    "        labels.extend(targets.numpy())  # Extract labels\n",
    "\n",
    "features_top = np.vstack(features_top)  # Stack top layer features\n",
    "features_bottom = np.vstack(features_bottom)  # Stack bottom layer features\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################\n",
    "\n",
    "print(features_top.shape)\n",
    "print(features_bottom.shape)\n",
    "colors_per_class = cm.rainbow(np.linspace(0, 1, 10))\n",
    "\n",
    "# Apply t-SNE to the features\n",
    "features_top_tsne = TSNE(n_components=2, init='pca', random_state=42).fit_transform(features_top)\n",
    "features_bottom_tsne = TSNE(n_components=2, init='pca', random_state=42).fit_transform(features_bottom)\n",
    "\n",
    "# Plot the t-SNE visualization\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Bottom Layer')\n",
    "for label in np.unique(labels):\n",
    "    plt.scatter(features_bottom_tsne[labels == label, 0], features_bottom_tsne[labels == label, 1], label=classes[label], s=5)\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Top Layer')\n",
    "for label in np.unique(labels):\n",
    "    plt.scatter(features_top_tsne[labels == label, 0], features_top_tsne[labels == label, 1], label=classes[label], s=5)\n",
    "plt.legend()\n",
    "plt.gcf().tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lv6rpKnWb9a1",
   "metadata": {
    "id": "lv6rpKnWb9a1"
   },
   "source": [
    "---\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b115708",
   "metadata": {
    "id": "3b115708"
   },
   "source": [
    "### Compute saliency map.\n",
    "\n",
    "A saliency map tells us the degree to which each pixel in the image affects the classification score for that image. To compute it, we compute the gradient of the unnormalized score corresponding to the correct class (which is a scalar) with respect to the pixels of the image.\n",
    "\n",
    "Read and understand the paper below, figure out how to compute saliency maps and implement it in the `compute_saliency_maps` function.\n",
    "\n",
    "[Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\", ICLR Workshop 2014.](https://arxiv.org/pdf/1312.6034.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fnQbAiU2OaGr",
   "metadata": {
    "id": "fnQbAiU2OaGr"
   },
   "outputs": [],
   "source": [
    "### helper function\n",
    "\n",
    "def show_saliency_maps(X, y, saliency):\n",
    "    # Compute saliency maps for images in X\n",
    "\n",
    "    # Convert the saliency map from Torch Tensor to numpy array and show images\n",
    "    # and saliency maps together.\n",
    "    if saliency.dim() == 4:\n",
    "      saliency = saliency.permute(0, 2, 3, 1).numpy()\n",
    "    elif saliency.dim() == 3:\n",
    "      saliency = saliency.numpy()\n",
    "    N = X.shape[0]\n",
    "    for i in range(N):\n",
    "        plt.subplot(2, N, i + 1)\n",
    "        img = np.transpose((X.detach()/2+0.5).numpy(),(0,2,3,1))\n",
    "        plt.imshow(img[i])\n",
    "        plt.axis('off')\n",
    "        plt.title(classes[y.detach().numpy()[i]])\n",
    "        plt.subplot(2, N, N + i + 1)\n",
    "        plt.imshow(saliency[i], cmap=plt.cm.hot)\n",
    "        plt.axis('off')\n",
    "        plt.gcf().set_size_inches(12, 5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlZRGEkCTiKO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "mlZRGEkCTiKO",
    "outputId": "ad010415-4e0f-492b-e74e-16e626d8d7d9"
   },
   "outputs": [],
   "source": [
    "### example images for saliency map and SmoothGrad visualization\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(test_loader)\n",
    "images,labels = next(dataiter)\n",
    "X = images[-3:,:,:,:]\n",
    "y = labels[-3:]\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(X))\n",
    "print('\\t'.join(f'{classes[y[j]]:5s}' for j in range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ckR-ky3riDiU",
   "metadata": {
    "id": "ckR-ky3riDiU"
   },
   "outputs": [],
   "source": [
    "def compute_saliency_maps(X, y, model):\n",
    "    \"\"\"\n",
    "    Compute a class saliency map using the model for images X and labels y.\n",
    "\n",
    "    Input:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; Tensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute the saliency map.\n",
    "\n",
    "    Returns:\n",
    "    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "\n",
    "    ##############################################################################\n",
    "    # TODO: Implement this function. Perform a forward and backward pass through #\n",
    "    # the model to compute the gradient of the correct class score with respect  #\n",
    "    # to each input image.                                                       #\n",
    "    ##############################################################################\n",
    "    # your code\n",
    "\n",
    "    # Set requires_grad attribute of tensor to be True\n",
    "    X_var = Variable(X, requires_grad=True)\n",
    "    y_var = Variable(y)\n",
    "\n",
    "    # Forward pass\n",
    "    scores = model(X_var)\n",
    "    scores = scores.gather(1, y_var.view(-1, 1)).squeeze()\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    scores.backward(torch.ones(scores.size()))\n",
    "\n",
    "    # Compute the saliency map\n",
    "    saliency = X_var.grad.data.abs()\n",
    "    saliency, _ = torch.max(saliency, dim=1)\n",
    "\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3oASx0dtoNH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "d3oASx0dtoNH",
    "outputId": "6d193000-5808-406d-f68e-093cd9a3f82d"
   },
   "outputs": [],
   "source": [
    "saliency = compute_saliency_maps(X.to(device), y.to(device), trained_model)\n",
    "show_saliency_maps(X, y, saliency.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NBzR2FDKqAo3",
   "metadata": {
    "id": "NBzR2FDKqAo3"
   },
   "source": [
    "In order to find an image that maximizes the correct score, we can perform gradient ascent on the input image, can we use the saliency map instead of the gradient we in each step to update the image. Is this assertion true? Why or why not?\n",
    "\n",
    "---\n",
    "**Write your answer and reason in this Markdown cell.**\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-TE9IaG0iD7E",
   "metadata": {
    "id": "-TE9IaG0iD7E"
   },
   "source": [
    "### Obtain improved visualiztion results by SmoothGrad.\n",
    "\n",
    "SmoothGrad is a method that can help visually sharpen gradient-based saliency maps thus improve the visulization quality. *[\"SmoothGrad: removing noise by adding noise\", ICML2017.](https://arxiv.org/pdf/1706.03825.pdf)*\n",
    "\n",
    "\n",
    "Read and understand the paper, implement SmoothGrad and apply **at least ONE** visualization technique mentioned in the paper in the following code block to get better results.\n",
    "\n",
    "You may also refer to this [blog](https://medium.com/@ML-STATS/reducing-noise-and-improving-interpretability-in-cnns-a-technical-review-of-the-smoothgrad-method-da648ee830c6) for concise illustration for SmoothGrad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y87Z85cvpLa_",
   "metadata": {
    "id": "y87Z85cvpLa_"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def compute_smoothgrad(X, y, model, num_samples=50, stdev_spread=0.15):\n",
    "    \"\"\"\n",
    "    Compute smoothed gradients for images in X given model\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input images; Tensor of shape (N, 3, H, W)\n",
    "    - y: Labels for X; Tensor of shape (N,)\n",
    "    - model: A pretrained CNN that will be used to compute gradients; see\n",
    "      the torchvision library\n",
    "    - num_samples: An integer; the number of gradient samples to compute for each\n",
    "      input in X.\n",
    "    - stdev_spread: A float; the standard deviation of the Gaussians used to\n",
    "      smooth the gradients.\n",
    "\n",
    "    Returns:\n",
    "    - smoothgrad: saliency: A Tensor of shape (N, 3, H, W) giving the smoothed saliency maps for the input\n",
    "    images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make sure the model is in \"test\" mode\n",
    "    model.eval()\n",
    "\n",
    "    ################################################################################\n",
    "    # TODO: Implement this function. Calculate SmoothGrad (smoothed saliency maps) #\n",
    "    #       based on the given parameters for this function.                       #\n",
    "    ################################################################################\n",
    "    # your code\n",
    "\n",
    "    # Initialize tensor to store smoothed gradients\n",
    "    smoothgrad = torch.zeros_like(X)\n",
    "\n",
    "    # Calculate gradients for each sample\n",
    "    for _ in range(num_samples):\n",
    "        # Add noise to input image\n",
    "        noise = torch.randn_like(X) * stdev_spread\n",
    "        X_noisy = X + noise\n",
    "\n",
    "        # Compute gradients for noisy input\n",
    "        X_var = Variable(X_noisy, requires_grad=True)\n",
    "        scores = model(X_var)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Add gradients to smoothgrad tensor\n",
    "        smoothgrad += X_var.grad.data\n",
    "\n",
    "    # Average gradients over samples\n",
    "    smoothgrad /= num_samples\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "\n",
    "    smoothgrad = (smoothgrad - smoothgrad.min()) / (smoothgrad.max() - smoothgrad.min()) # just for better visualization\n",
    "    return smoothgrad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KQqh_Ek5Mccz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "KQqh_Ek5Mccz",
    "outputId": "92aae365-85d3-43c8-f76b-3c764931e03e"
   },
   "outputs": [],
   "source": [
    "smoothgrad = compute_smoothgrad(X.to(device), y.to(device), trained_model)\n",
    "show_saliency_maps(X, y, smoothgrad.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V4HQKHuyU6pG",
   "metadata": {
    "id": "V4HQKHuyU6pG"
   },
   "source": [
    "Compare the results of saliency map and SmoothGrad, what is your discovery? Try to understand the papers to give a reason for the phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0xrm4anOU0yF",
   "metadata": {
    "id": "0xrm4anOU0yF"
   },
   "source": [
    "---\n",
    "\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80U7JYr7S62Z",
   "metadata": {
    "id": "80U7JYr7S62Z"
   },
   "source": [
    "### Design and conduct experiment  to explore and explain how CNN works.\n",
    "\n",
    "Design and conduct one experiment beyond the example images above by utilizing ```compute_smoothgrad```, e.g., compare the SmoothGrad maps of different classes given the same model, compare the SmoothGrad maps of the same class for different models, etc. Please quanlitatively show some evidence (e.g., plotting some examplar images clearly and elegantly) with necessary code snippets, write down your observations and briefly explain.\n",
    "\n",
    "For the experiment you design, please specify:\n",
    "1. What question do you intend to study?\n",
    "2. To study the proposed question, how do you design your experiment?\n",
    "3. What conclusion do you make from the experiment results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X_dX5HbNToJv",
   "metadata": {
    "id": "X_dX5HbNToJv"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TO DO: Design and conduct your experiment.                                 #\n",
    "##############################################################################\n",
    "# your code\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hOwuKz6-UcX2",
   "metadata": {
    "id": "hOwuKz6-UcX2"
   },
   "source": [
    "---\n",
    "\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77141694",
   "metadata": {
    "id": "77141694"
   },
   "source": [
    "### Fooling image\n",
    "We can also use image gradients to generate \"fooling images\", that is, given an image and a target class, we can perform gradient ascent over the image to maximize the target class, stopping when the network classifies the image as the target class. Implement the following function to generate fooling images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be6ca2e",
   "metadata": {
    "id": "1be6ca2e"
   },
   "outputs": [],
   "source": [
    "def make_fooling_image(X, target_y, model):\n",
    "    \"\"\"\n",
    "    Generate a fooling image that is close to X, but that the model classifies\n",
    "    as target_y.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input image; Tensor of shape (1, 3, H, W)\n",
    "    - target_y: An integer in the range [0, 10)\n",
    "    - model: A pretrained CNN\n",
    "\n",
    "    Returns:\n",
    "    - X_fooling: An image that is close to X, but that is classifed as target_y\n",
    "    by the model.\n",
    "    \"\"\"\n",
    "    # Initialize our fooling image to the input image, and make it require gradient\n",
    "    X_fooling = X.clone()\n",
    "    X_fooling = X_fooling.requires_grad_()\n",
    "\n",
    "    learning_rate = 1\n",
    "    ##############################################################################\n",
    "    # TODO: Generate a fooling image X_fooling that the model will classify as   #\n",
    "    # the class target_y. You should perform gradient ascent on the score of the #\n",
    "    # target class, stopping when the model is fooled.                           #\n",
    "    # When computing an update step, first normalize the gradient:               #\n",
    "    #   dX = learning_rate * g / ||g||_2                                         #\n",
    "    #                                                                            #\n",
    "    # You should write a training loop.                                          #\n",
    "    #                                                                            #\n",
    "    # HINT: For most examples, you should be able to generate a fooling image    #\n",
    "    # in fewer than 100 iterations of gradient ascent.                           #\n",
    "    # You can print your progress over iterations to check your algorithm.       #\n",
    "    ##############################################################################\n",
    "    \n",
    "    # your code\n",
    "\n",
    "    num_iterations = 100  # Number of iterations for gradient ascent\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Forward pass to get the scores\n",
    "        scores = model(X_fooling)\n",
    "        # Calculate the gradient of the target class score with respect to the input\n",
    "        target_score = scores[0, target_y]\n",
    "        target_score.backward()\n",
    "\n",
    "        # Normalize the gradient\n",
    "        with torch.no_grad():\n",
    "            # Compute the L2 norm of the gradient\n",
    "            grad_norm = X_fooling.grad.norm()\n",
    "            # Update the fooling image using gradient ascent\n",
    "            X_fooling.data += learning_rate * X_fooling.grad / grad_norm\n",
    "\n",
    "        # Reset the gradient for the next iteration\n",
    "        X_fooling.grad.zero_()\n",
    "\n",
    "        # Check if the model is fooled\n",
    "        fooled_y = torch.argmax(model(X_fooling), dim=1)\n",
    "        if fooled_y.item() == target_y:\n",
    "            print(f\"Model was fooled after {i + 1} iterations\")\n",
    "            break\n",
    "\n",
    "\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return X_fooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81281231",
   "metadata": {
    "id": "81281231"
   },
   "outputs": [],
   "source": [
    "target_y = 6 # label 'frog'\n",
    "image_to_be_fooled = images[-1:,:,:,:] # an image of plane\n",
    "y = labels[-1:] # label 'plane'\n",
    "\n",
    "X_fooling = make_fooling_image(image_to_be_fooled.to(device), target_y, trained_model.to(device))\n",
    "\n",
    "scores = trained_model(X_fooling)\n",
    "assert target_y == scores.data.max(1)[1][0].item(), 'The model is not fooled!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797f3cd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "797f3cd5",
    "outputId": "66df9102-a70a-4547-f8ed-71b31157bf17"
   },
   "outputs": [],
   "source": [
    "org_img = (torch.squeeze(image_to_be_fooled, dim=0).detach().cpu().numpy()) / 2 + 0.5\n",
    "fooling_img = (torch.squeeze(X_fooling, dim = 0).detach().cpu().numpy()) / 2 + 0.5\n",
    "\n",
    "fooling_img.astype(np.uint8)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(org_img.transpose((1, 2, 0)))\n",
    "plt.title(classes[y])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(fooling_img.transpose((1, 2, 0)))\n",
    "plt.title(classes[target_y])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow((org_img - fooling_img).transpose((1, 2, 0)))\n",
    "plt.title('difference')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(10*(org_img - fooling_img).transpose((1, 2, 0)))\n",
    "plt.title('magnified_difference')\n",
    "plt.axis('off')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b8082",
   "metadata": {
    "id": "hOwuKz6-UcX2"
   },
   "source": [
    "Observe the results above and write down your discovery.\n",
    "\n",
    "---\n",
    "\n",
    "**Write your observations and analysis in this Markdown cell:**\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "65696248c9b71dd3c48f2dd2e5b2d3607d9aed1eef8791c3764510e063b61a82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
